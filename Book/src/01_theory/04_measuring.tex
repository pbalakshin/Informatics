\subsection{Измерение количества информации}

Любые сообщения измеряются в \emph{байтах}, \emph{килобайтах}, \emph{мегабайтах}, \emph{гигабайтах}, \emph{терабайтах}, \emph{петабайтах} и \emph{эксабайтах}, а кодируются, например, в компьютере, с помощью \emph{алфавита} из нулей и единиц, записываются и реализуются в ЭВМ в \emph{битах}.

Приведем основные соотношения между единицами измерения \emph{сообщений}:
\begin{itemize}[noitemsep]
    \item 1 бит (\textbf{bi}nary digi\textbf{t} - двоичное число) = 0 или 1;
    \item 1 байт = 8 бит;
    \item 1 килобайт (1 Кб) = $2^{13}$ бит;
    \item 1 мегабайт (1 Мб) = $2^{23}$ бит;
    \item 1 гигабайт (1 Гб) = $2^{33}$ бит;
    \item 1 терабайт (1 Тб) = $2^{43}$ бит;
    \item 1 петабайт (1 Пб) = $2^{53}$ бит;
    \item 1 эксабайт (1 Эб) = $2^{63}$ бит.
\end{itemize}

Теперь нам известно понятие информации, но необходимо еще конкретно знать сколько этой информации. Поэтому есть два важных определения:
\begin{description}
    \item [Количество информации] --- число, адекватно характеризующее разнообразие (структурированность, определённость,выбор состояний и т.д.) в оцениваемой системе. Количество информации часто оценивается в битах, причем такая оценка может выражаться и в долях бит (так как речь идет не об измерении или кодировании сообщений).
    \item [Мера информации] --- численная оценка количества информации, которая обычно задана неотрицательной, определенной на множестве событий и являющейся аддитивной функцией (то есть, мера информации объединения событий (множеств) равна сумме мер каждого события). Заметим, что функция меры информации монотонна (при уменьшении или увеличении вероятности некоторого события количество иноформации в системе монотонно уменьшается или увеличивается). 
    \\\textbf{Важно:} мера вероятности всегда находится в диапазоне от 0 до 1
\end{description}

Для измерения информации используются различные подходы и методы, например, с использованием меры информации по Р. Хартли и К. Шеннону.

\subsubsection{Мера Хартли}

% \noindent
\begin{minipage}[t]{\textwidth}
\begin{wrapfigure}{l}{0.25\textwidth}
    \centering
    \vspace{-\intextsep}
    \includegraphics[width=0.25\textwidth]{person/hartley-ralph}
    \caption*{Ральф Хартли\\1888 -- 1970}
\end{wrapfigure}

Пусть известны $N$ состояний системы $S$ ($N$  опытов с различными, равновозможными, последовательными состояниями системы). Если каждое состояние системы закодировать двоичными кодами, то минимальная длина $d$ полученного кода определяется из условия:

$$
2^{d} \ge N \qquad \mbox{\emph{или}} \qquad  d \ge \log_{2}N
$$

Значит, для однозначного описания системы требуется $\log_{2}N$ бит. В общем случае количество информации в системе $S$ равно:
$$
H_{s} = \log_{k}N
$$
\end{minipage}

\bigskip
Единицы измерения количества информации:
\begin{itemize}[noitemsep]
  \item Бит ($k = 2$)
  \item Трит ($k = 3$)
  \item Дит (харт) ($k = 10$)
  \item Нит (нат) ($k = e$)
\end{itemize}

\paragraph{Примеры использования меры Хартли}

\example{1}

\task мальчик загадывает число от 1 до 64. Какое количество вопросов типа ``да-нет'' понадобится, чтобы гарантированно угадать число?
\solution
\begin{itemize}[noitemsep]
    \item Первый вопрос: ``Загаданное число меньше 32?''. Ответ: ``Да''.
    \item Второй вопрос: ``Загаданное число меньше 16?''. Ответ: ``Нет''.
    \item [] \dots
    \item Шестой вопрос точно приведет к правильному ответу.
\end{itemize}

\noindent
Значит, в соответствии с мерой Хартли в загадке мальчика содержится $\log_{2}64 = 6$ бит информации ($N = 64$ так как возможно 64 вариантов загаданного числа).
\answer 6 бит.


\example{2}
\task Мальчик держит за спиной шахматного ферзя и собирается поставить его на произвольную клетку пустой доски. Какое количество информации содержится в его действии?
\solution Шахматная доска имеет размеры $8\times 8$ клеток.
Ферзь может быть как белым, так и черным, поэтому количество равновероятных состояний будет равно $8 \times 8 \times 2 = 128$.
Получается, количество информации по мере Хартли равно $\log_{2}128 = 7$ бит.
\answer 7 бит.

\bigskip
Если во множестве $X = {x_1,x_2, ..., x_n}$ искать произвольный элемент, то для его нахождения (по Хартли) необходимо иметь не менее $\log_{a}n$ (единиц) информации. 

Уменьшение $H$ говорит об уменьшении разнообразия состояний $N$ системы, а увеличение $H$ говорит об увеличении разнообразия состояний $N$ системы.

Мера Хартли подходит лишь для идеальных, абстрактных систем, так как в реальных системах состояния системы неодинаково осуществимы (неравновероятны).

\subsubsection{Мера Шеннона}

\begin{minipage}{\textwidth}
\begin{wrapfigure}{l}{0.25\textwidth}
    \centering
    \includegraphics[width=0.25\textwidth]{person/shannon-claude}
    \caption*{Клод Шеннон\\1916 -- 2001}
\end{wrapfigure}

Если состояния системы не равновероятны, используют меру Шеннона. Мера Шеннона оценивает информацию отвлеченно от ее смысла:
$$I = - \sum^{N}_{i=1}p_{i}\times \log_{2}{p_{i}}$$ где:
\begin{description}[noitemsep]
    \item [$I$] -- количество информации, выраженное в битах (в $\log_{k}p_{i}$ $k = 2$);
    \item [$N$] --- число состояний системы;
    \item [$p_{i}$] --- вероятность (относительная частота) перехода системы в $i$-е состояние (вероятность того, что система находится в состоянии $i$)
\end{description}
\end{minipage}

\bigskip
Сумма всех $p_{i}$ должна быть равна единице.

Если все состояния рассматриваемой системы равновозможны, равновероятны, то есть $p_i = 1/n$, то из \emph{формулы Шеннона} можно получить (как частный случай) \emph{формулу Хартли}:
$$I = \log_{2}{n}$$

Обозначим величину:
$$f_i = -n\log_{2}{p_i}$$

Тогда из \emph{формулы К. Шеннона} следует, что количество информации I можно понимать как среднеарифметическое величин $f_i$ , то есть величину $f_i$ можно интерпретировать как \emph{информационное содержание символа алфавита} с индексом i и величиной $p_i$ вероятности появления этого символа в любом сообщении (слове), передающем информацию.

В термодинамике известен так называемый коэффициент Больцмана $k = 1.38 * 10^{-16} \text{(эрг. град)}$ и выражение (\emph{формула Больцмана}) для энтропии или меры хаоса в термодинамической системе:
$$S = -k \sum^{N}_{i=1}p_{i}\times \ln{p_{i}}$$

Сравнивая выражения для I и S, можно заключить, что величину I можно понимать как энтропию из-за нехватки информации в системе (о системе).

Формулы энтропии и информации идентичны, но смысл разный. Энтропия априорная характеристика (до передачи), информация – апостериорная (после передачи).

Из этой формулы следуют важные выводы:
\begin{itemize}
    \item увеличение меры Шеннона свидетельствует об уменьшении энтропии (увеличении порядка) системы;
    \item уменьшение меры Шеннона свидетельствует об увеличении энтропии (увеличении беспорядка) системы.
\end{itemize}

Положительная сторона \emph{формулы Шеннона} --- ее отвлеченность от смысла информации. Кроме того, в отличие от \emph{формулы Хартли}, она учитывает различность состояний, что делает ее пригодной для практических вычислений. Основная отрицательная сторона \emph{формулы Шеннона} – она не распознает различные состояния системы с одинаковой вероятностью.

\paragraph{Примеры использования меры Шеннона}

\example{1}
\task девочка наугад вытаскивает из мешка мяч. Известно, что в мешке всего 8 мячей, из них: 4 красных, 2 синих, 1 зеленый и 1 белый. Какое количество информации содержится в этом событии?
\solution
\begin{itemize}[noitemsep]
    \item Вероятность вытащить красный мяч равна $4 / 8 = 0,5$
    \item Вероятность вытащить синий мяч равна $4/8 = 0,25$
    \item Вероятность вытащить зеленый мяч равна $1/8 = 0,125$
    \item Вероятность вытащить белый мяч равна $1/8 = 0,125$
\end{itemize}

\noindentЗначит количество информации, выраженное в битах равно:
\begin{flalign*}
    I = {} & -( \frac{1}{2} \times \log_{2}\frac{4}{8}  + \frac{1}{4} \times \log_{2}\frac{1}{8} + \frac{1}{8} \times \log_{2}\frac{1}{8} + \frac{1}{8} \times \log_{2}\frac{1}{8})  \\
     & = -(-0,5 \times 1 - 0,25 \times 2 - 0,125 \times 3 - 0,125 \times 3) \\
     & =  -(-0,5 - 0,5 - 0,375 - 0,375) \\
     & = 1,7 \mbox{ бит}
\end{flalign*}

\answer 1,75 бит