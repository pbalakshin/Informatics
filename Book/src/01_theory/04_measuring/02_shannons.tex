\subsubsection{Мера Шеннона}

\begin{wrapfigure}{l}{0.21\textwidth}
    \centering
    \includegraphics[width=0.2\textwidth]{shannon}
    \caption*{Клод Шеннон\\1916 -- 2001}
\end{wrapfigure}

Если состояния системы не равновероятны, используют меру Шеннона. Мера Шеннона оценивает информацию отвлеченно от ее смысла:
$$I = - \sum^{N}_{i=1}p_{i}\times \log_{2}p_{i},$$ где:
\begin{description}[noitemsep]
    \item [$I$] -- количество информации, выраженное в битах (в $\log_{k}p_{i}$ $k = 2$);
    \item [$N$] --- число состояний системы;
    \item [$p_{i}$] --- вероятность (относительная частота) перехода системы в $i$-е состояние (вероятность того, что система находится в состоянии $i$)
\end{description}

Сумма всех $p_{i}$ должна быть равна единице.

Если все состояния рассматриваемой системы равновозможны, равновероятны, то есть $p_i = 1/n$, то из \emph{формулы Шеннона} можно получить (как частный случай) \emph{формулу Хартли}:
$$I = \log_{2}n.$$

Обозначим величину:
$$f_i = -n\log_{2}p_i.$$

Тогда из \emph{формулы К. Шеннона} следует, что количество информации I можно понимать как среднеарифметическое величин $f_i$ , то есть величину $f_i$ можно интерпретировать как \emph{информационное содержание символа алфавита} с индексом i и величиной $p_i$ вероятности появления этого символа в любом сообщении (слове), передающем информацию.

В термодинамике известен так называемый коэффициент Больцмана $k = 1.38 * 10^{-16} \mbox{(эрг.град)}$ и выражение (\emph{формула Больцмана}) для энтропии или меры хаоса в термодинамической системе:
$$
S = -k \sum^{N}_{i=1}p_{i}\times \ln{p_{i}}
$$

Сравнивая выражения для I и S, можно заключить, что величину I можно понимать как энтропию из-за нехватки информации в системе (о системе).

Формулы энтропии и информации идентичны, но смысл разный. Энтропия априорная характеристика (до передачи), информация – апостериорная (после передачи).

Из этой формулы следуют важные выводы:
\begin{itemize}
    \item увеличение меры Шеннона свидетельствует об уменьшении энтропии (увеличении порядка) системы;
    \item уменьшение меры Шеннона свидетельствует об увеличении энтропии (увеличении беспорядка) системы.
\end{itemize}

Положительная сторона \emph{формулы Шеннона} --- ее отвлеченность от смысла информации. Кроме того, в отличие от \emph{формулы Хартли}, она учитывает различность состояний, что делает ее пригодной для практических вычислений. Основная отрицательная сторона \emph{формулы Шеннона} – она не распознает различные состояния системы с одинаковой вероятностью.

\paragraph{Примеры использования меры Шеннона}

\example{1}

\task девочка наугад вытаскивает из мешка мяч. Известно, что в мешке всего 8 мячей, из них: 4 красных, 2 синих, 1 зеленый и 1 белый. Какое количество информации содержится в этом событии?

\solution
\begin{itemize}
    \item Вероятность вытащить красный мяч равна $\displaystyle\frac{4}{8} = 0,5$
    \item Вероятность вытащить синий мяч равна $\displaystyle\frac{4}{8} = 0,25$
    \item Вероятность вытащить зеленый мяч равна $\displaystyle\frac{1}{8} = 0,125$
    \item Вероятность вытащить белый мяч равна $\displaystyle\frac{1}{8} = 0,125$
\end{itemize}

\noindentЗначит количество информации, выраженное в битах равно:
\begin{flalign*}
I = {} & -( \frac{1}{2} \times \log_{2}\frac{4}{8}  + \frac{1}{4} \times \log_{2}\frac{1}{8} + \frac{1}{8} \times \log_{2}\frac{1}{8} + \frac{1}{8} \times \log_{2}\frac{1}{8})  \\
         & = -(-0,5 \times 1 - 0,25 \times 2 - 0,125 \times 3 - 0,125 \times 3) \\
         & =  -(-0,5 - 0,5 - 0,375 - 0,375) \\
         & = 1,7 \mbox{ бит}
\end{flalign*}
% $I = -(0,5 \times \log_{2}0,5  + 0,25 \times \log_{2}0,25 + 0,125 \times \log_{2}0,125 + 0,125 \times \log_{2}0,125) = -(-0,5\times 1 - 0,25\times 2 - 0,125\times 3 - 0,125\times 3) = -(-0,5 - 0,5\hm - 0,375\hm - 0,375\hm) = 1,75$ бит.

\answer 1,75 бит