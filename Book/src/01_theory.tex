% !TeX root = ../main.tex
\section{Основы теории информации}
\label{sec:theory}

\subsection{Терминология информатики}
\label{subsec:computer-science-terminology}

Начать изучение информатики невозможно, не разобравшись в точном значении термина «информатика». Однако, до сих пор в мировой научной общественности не сложилось четкого понимания этого термина. Рассмотрим одно из популярных определений:

\textbf{Информатика} --- дисциплина, изучающая свойства и структуру информации, закономерности ее создания, преобразования, накопления, передачи и использования. За рубежом сложилась чуть более узкая трактовка термина информатика. Там под этим понимают пересечение сразу трех областей науки – это информационные технологии, теория информации и computer science. Всё обозначенное выше подходит под определение самого курса ``Информатика''.

Изучая некоторую науку важно представлять основные даты, вехи её развития:
\begin{itemize}
    \item 1956(57) --- появление термина <<информатика>> (\textit{нем.} Informatik, Штейнбух).
    \item 1968 --- первое упоминание в СССР (информология, Харкевич).
    \item 197Х --- информатика стала отдельной наукой.
    \item 4 декабря --- день российской информатики.
\end{itemize}

\subsection{Терминология теории информации}
\label{subsec:information-theory-terminology}

Рассмотрим некоторые терминологические тонкости. В обыденном языке, слова ``информация'' и <<данные>> считаются синонимами. Они, как правило, употребляются взаимозаменяемо. И так обстоит дело в информатике и в целом, в компьютерных науках.

Понятие \textit{``информация''} имеет различные трактовки в различных предметных областях. Например, \textit{информация} может пониматься как:
\begin{itemize}[noitemsep]
    \item сигналы для управления, приспособления рассматриваемой системы (в кибернетике);
    \item мера хаоса в рассматриваемой системе (в физике);
    \item вероятность выбора в рассматриваемой системе (в теории вероятностей);
    \item мера разнообразия в рассматриваемой системе (в биологии) и др.
\end{itemize}
Но мы остановимся на понятиях, близких к информатике.

\begin{description}
    \item [Информация] --- это некоторая упорядоченная последовательность сообщений, отражающих, передающих и увеличивающих наши знания.
    \item [Информация] --- это сведения об окружающем мире (объекте, процессе, явлении, событии), которые являются объектом преобразования (включая хранение, передачу и т.д.) и используются для выработки поведения, для принятия решения, для управления или для обучения.
    \item [Информация] --- это новые сведения, подлежащие передаче, хранению и обработке.
\end{description}

Рассмотрим это фундаментальное понятие информатики на основе понятия \textit{``алфавит''} (``алфавитный'', формальный подход). Дадим формальное определение \textit{алфавита}.

\begin{description}
    \item [Алфавит] --- конечное множество различных знаков (букв), символов, для которых определена операция \emph{конкатенации} (присоединения символа к символу или цепочке символов); с ее помощью по определенным правилам соединения символов и слов можно получать слова (цепочки знаков) и словосочетания (цепочки \textit{слов}) в этом \textit{алфавите} (над этим \textit{алфавитом}).
    \item [Знак (буква)] --- любой элемент алфавита (элемент $x$ алфавита $X$, где $x \in X$). Понятие знака неразрывно связано с тем, что им обозначается (``со смыслом''), они вместе могут рассматриваться как пара элементов ($x$, $y$), где $x$ – сам знак, а $y$ – обозначаемое этим знаком.
\end{description}

\example{1}
\noindent
Примеры \emph{алфавитов:} множество из десяти цифр, множество из знаков русского языка, точка и тире в азбуке Морзе и др. В \emph{алфавите} цифр знак 5 связан с понятием ``быть в количестве пяти элементов''.

\textbf{Слово} в алфавите (или над алфавитом) - конечная последовательность знаков (букв) алфавита.

\textbf{Длина} |p| некоторого слова $p$ в алфавите (над алфавитом) - число составляющих его букв.

\textbf{Словарь (словарный запас)} - множество различных слов в алфавите (над алфавитом).
В отличие от конечного \emph{алфавита}, словарный запас может быть и бесконечным.
\emph{Слова} над некоторым заданным \emph{алфавитом} и определяют так называемые \emph{сообщения}.

\example{2}
\noindent
\emph{Слова} над \emph{алфавитом} кириллицы --- ``Информатика'',``инто'', ``ииии'', ``и''.

\noindent
\emph{Слова} над \emph{алфавитом} десятичных цифр и знаков арифметических операций --- ``1256'', ``23+78'', ``35–6+89'', ``4''.

\noindent
\emph{Слова} над \emph{алфавитом} азбуки Морзе --- ``.'', ``. . –'', ``– – –''.

В \emph{алфавите} должен быть определен порядок следования \emph{букв} (порядок типа ``предыдущий элемент --- последующий элемент''), то есть любой \emph{алфавит} имеет упорядоченный вид $X = {x_1, x_2, \ldots, x_n}$ .

Таким образом, \emph{алфавит} должен позволять решать задачу лексикографического (алфавитного) упорядочивания, или задачу расположения \emph{слов} над этим \emph{алфавитом}, в соответствии с порядком, определенным в \emph{алфавите} (то есть по символам \emph{алфавита}).

\subsection{Признаки классификации информации}

Рассмотрим две классификации информации. Первая из них --- классификация по форме \emph{сообщений} --- определенного вида сигналов, символов:
\begin{itemize}[noitemsep]
    \item отношение к источнику или приемнику (входная, выходная и внутренняя);
    \item отношение к конечному результату (исходная, промежуточная и результирующая);
    \item актуальность;
    \item адекватность;
    \item доступность (открытая, закрытая);
    \item понятность;
    \item полнота (достаточная, недостаточная, избыточная);
    \item достоверность;
    \item массовость;
    \item изменчивость (постоянная, переменная, смешанная);
    \item объективность;
    \item точность;
    \item стадия использования (первичная, вторичная);
    \item ценность.
\end{itemize}

Вторая классификация --- по форме преставления информации, способам ее кодирования и хранения:
\begin{itemize}[noitemsep]
    \item графическая;
    \item звуковая;
    \item текстовая;
    \item числовая;
    \item видеоинформация.
\end{itemize}

\subsection{Измерение количества информации}

Любые сообщения измеряются в \emph{байтах}, \emph{килобайтах}, \emph{мегабайтах}, \emph{гигабайтах}, \emph{терабайтах}, \emph{петабайтах} и \emph{эксабайтах}, а кодируются, например, в компьютере, с помощью \emph{алфавита} из нулей и единиц, записываются и реализуются в ЭВМ в \emph{битах}.

Приведем основные соотношения между единицами измерения \emph{сообщений}:
\begin{itemize}[noitemsep]
    \item 1 бит (\textbf{bi}nary digi\textbf{t} - двоичное число) = 0 или 1;
    \item 1 байт = 8 бит;
    \item 1 килобайт (1 Кб) = $2^{13}$ бит;
    \item 1 мегабайт (1 Мб) = $2^{23}$ бит;
    \item 1 гигабайт (1 Гб) = $2^{33}$ бит;
    \item 1 терабайт (1 Тб) = $2^{43}$ бит;
    \item 1 петабайт (1 Пб) = $2^{53}$ бит;
    \item 1 эксабайт (1 Эб) = $2^{63}$ бит.
\end{itemize}

Теперь нам известно понятие информации, но необходимо еще конкретно знать сколько этой информации. Поэтому есть два важных определения:
\begin{description}
    \item [Количество информации] --- число, адекватно характеризующее разнообразие (структурированность, определённость,выбор состояний и т.д.) в оцениваемой системе. Количество информации часто оценивается в битах, причем такая оценка может выражаться и в долях бит (так как речь идет не об измерении или кодировании сообщений).
    \item [Мера информации] --- численная оценка количества информации, которая обычно задана неотрицательной, определенной на множестве событий и являющейся аддитивной функцией (то есть, мера информации объединения событий (множеств) равна сумме мер каждого события). Заметим, что функция меры информации монотонна (при уменьшении или увеличении вероятности некоторого события количество иноформации в системе монотонно уменьшается или увеличивается).
          \\\textbf{Важно:} мера вероятности всегда находится в диапазоне от 0 до 1
\end{description}

Для измерения информации используются различные подходы и методы, например, с использованием меры информации по Р. Хартли и К. Шеннону.

\subsubsection{Мера Хартли}

\begin{minipage}[t]{\textwidth}
    \begin{wrapfigure}{l}{0.25\textwidth}
        \centering
        \vspace{-\intextsep}
        \includegraphics[width=0.25\textwidth]{person/hartley-ralph}
        \caption*{Ральф Хартли\\1888 -- 1970}
    \end{wrapfigure}

    Пусть известны $N$ состояний системы $S$ ($N$  опытов с различными, равновозможными, последовательными состояниями системы). Если каждое состояние системы закодировать двоичными кодами, то минимальная длина $d$ полученного кода определяется из условия:
    \[2^{d} \ge N \qquad \text{\emph{или}} \qquad  d \ge \log_{2}N\]

    Значит, для однозначного описания системы требуется $\log_{2}N$ бит. В общем случае количество информации в системе $S$ равно: \\
    $H_{s} = \log_{k}N$

\end{minipage}

\bigskip
Единицы измерения количества информации:
\begin{itemize}[noitemsep]
    \item Бит ($k = 2$)
    \item Трит ($k = 3$)
    \item Дит (харт) ($k = 10$)
    \item Нит (нат) ($k = e$)
\end{itemize}

\paragraph{Примеры использования меры Хартли}

\example{1}
\task{} мальчик загадывает число от 1 до 64. Какое количество вопросов типа ``да-нет'' понадобится, чтобы гарантированно угадать число?
\solution{}
\begin{itemize}[noitemsep]
    \item Первый вопрос: ``Загаданное число меньше 32?''. Ответ: ``Да''.
    \item Второй вопрос: ``Загаданное число меньше 16?''. Ответ: ``Нет''.
    \item [] \dots
    \item Шестой вопрос точно приведет к правильному ответу.
\end{itemize}

\noindent
Значит, в соответствии с мерой Хартли в загадке мальчика содержится $\log_{2}64 = 6$ бит информации ($N = 64$ так как возможно 64 вариантов загаданного числа).
\answer{} 6 бит.

\example{2}
\task{} Мальчик держит за спиной шахматного ферзя и собирается поставить его на произвольную клетку пустой доски. Какое количество информации содержится в его действии?
\solution{} Шахматная доска имеет размеры $8\times 8$ клеток.
Ферзь может быть как белым, так и черным, поэтому количество равновероятных состояний будет равно $8 \times 8 \times 2 = 128$.
Получается, количество информации по мере Хартли равно $\log_{2}128 = 7$ бит.
\answer{} 7 бит.

\bigskip
Если во множестве $X = {x_1,x_2, ..., x_n}$ искать произвольный элемент, то для его нахождения (по Хартли) необходимо иметь не менее $\log_{a}n$ (единиц) информации.

Уменьшение $H$ говорит об уменьшении разнообразия состояний $N$ системы, а увеличение $H$ говорит об увеличении разнообразия состояний $N$ системы.

Мера Хартли подходит лишь для идеальных, абстрактных систем, так как в реальных системах состояния системы неодинаково осуществимы (неравновероятны).

\subsubsection{Мера Шеннона}

\begin{minipage}{\textwidth}
    \begin{wrapfigure}{l}{0.25\textwidth}
        \centering
        \includegraphics[width=0.25\textwidth]{person/shannon-claude}
        \caption*{Клод Шеннон\\1916 -- 2001}
    \end{wrapfigure}

    Если состояния системы не равновероятны, используют меру Шеннона. Мера Шеннона оценивает информацию отвлеченно от ее смысла:
    \[I = - \sum^{N}_{i=1}p_{i}\times \log_{2}{p_{i}}\] где:
    \begin{description}[noitemsep]
        \item [$I$] -- количество информации, выраженное в битах (в $\log_{k}p_{i}$ $k = 2$);
        \item [$N$] --- число состояний системы;
        \item [$p_{i}$] --- вероятность (относительная частота) перехода системы в $i$-е состояние (вероятность того, что система находится в состоянии $i$)
    \end{description}
\end{minipage}

\bigskip
Сумма всех $p_{i}$ должна быть равна единице.

Если все состояния рассматриваемой системы равновозможны, равновероятны, то есть $p_i = 1/n$, то из \emph{формулы Шеннона} можно получить (как частный случай) \emph{формулу Хартли}:
\[I = \log_{2}{n}\]

Обозначим величину:
\[f_i = -n\log_{2}{p_i}\]

Тогда из \emph{формулы К. Шеннона} следует, что количество информации I можно понимать как среднеарифметическое величин $f_i$ , то есть величину $f_i$ можно интерпретировать как \emph{информационное содержание символа алфавита} с индексом i и величиной $p_i$ вероятности появления этого символа в любом сообщении (слове), передающем информацию.

В термодинамике известен так называемый коэффициент Больцмана $k = 1.38 * 10^{-16} \text{(эрг. град)}$ и выражение (\emph{формула Больцмана}) для энтропии или меры хаоса в термодинамической системе:
\[S = -k \sum^{N}_{i=1}p_{i}\times \ln{p_{i}}\]

Сравнивая выражения для I и S, можно заключить, что величину I можно понимать как энтропию из-за нехватки информации в системе (о системе).

Формулы энтропии и информации идентичны, но смысл разный. Энтропия априорная характеристика (до передачи), информация – апостериорная (после передачи).

Из этой формулы следуют важные выводы:
\begin{itemize}
    \item увеличение меры Шеннона свидетельствует об уменьшении энтропии (увеличении порядка) системы;
    \item уменьшение меры Шеннона свидетельствует об увеличении энтропии (увеличении беспорядка) системы.
\end{itemize}

Положительная сторона \emph{формулы Шеннона} --- ее отвлеченность от смысла информации. Кроме того, в отличие от \emph{формулы Хартли}, она учитывает различность состояний, что делает ее пригодной для практических вычислений. Основная отрицательная сторона \emph{формулы Шеннона} – она не распознает различные состояния системы с одинаковой вероятностью.

\paragraph{Примеры использования меры Шеннона}

\example{1}
\task{} девочка наугад вытаскивает из мешка мяч. Известно, что в мешке всего 8 мячей, из них: 4 красных, 2 синих, 1 зеленый и 1 белый. Какое количество информации содержится в этом событии?
\solution{}
\begin{itemize}[noitemsep]
    \item Вероятность вытащить красный мяч равна $4 / 8 = 0,5$
    \item Вероятность вытащить синий мяч равна $4/8 = 0,25$
    \item Вероятность вытащить зеленый мяч равна $1/8 = 0,125$
    \item Вероятность вытащить белый мяч равна $1/8 = 0,125$
\end{itemize}

\noindent
Значит количество информации, выраженное в битах равно:
\begin{flalign*}
    I & =  -( \frac{1}{2} \times \log_{2}\frac{4}{8}  + \frac{1}{4} \times \log_{2}\frac{1}{8} + \frac{1}{8} \times \log_{2}\frac{1}{8} + \frac{1}{8} \times \log_{2}\frac{1}{8})  \\
     & = -(-0,5 \times 1 - 0,25 \times 2 - 0,125 \times 3 - 0,125 \times 3) \\
     & =  -(-0,5 - 0,5 - 0,375 - 0,375) \\
     & = 1,7 \text{ бит}
\end{flalign*}

\answer{} 1,75 бит
\subsection{Методы получения информации}

Методы получения информации можно разбить на три большие группы:

\begin{itemize}[noitemsep]
    \item \emph{Эмпирические};
    \item \emph{Теоретические};
    \item \emph{Эмпирико-теоретические}.
\end{itemize}

Кратко рассмотрим и охарактеризуем все три метода по отдельности.

\subsubsection{Эмпирические методы}

Эмпирические методы или методы получения эмпирических данных.

\begin{description}[noitemsep]
    \item [Наблюдение] --- сбор первичной информации об объекте, процессе, явлении.
    \item [Сравнение] --- обнаружение и соотнесение общего и различного.
    \item [Измерение] --- поиск с помощью измерительных приборов эмпирических фактов.
    \item [Эксперимент] --- преобразование, рассмотрение объекта, процесса, явления с целью выявления каких-то новых свойств.
\end{description}

\noindent
Кроме классических форм их реализации, в последнее время используются опрос, интервью, тестирование и другие.

\subsubsection{Теоретические методы}

Теоретические методы или методы построения различных теорий.

\begin{description}
    \item [Восхождение от абстрактного к конкретному] --- получение знаний о целом или о его частях на основе знаний об абстрактных проявлениях в сознании, в мышлении.
    \item [Идеализация] --- получение знаний о целом или его частях путем представления в мышлении целого или частей, не существующих в действительности.
    \item [Формализация] --- получение знаний о целом или его частях с помощью языков искусственного происхождения (формальное описание, представление).
    \item [Аксиоматизация] --- получение знаний о целом или его частях с помощью некоторых аксиом (не доказываемых в данной теории утверждений) и правил получения из них (и из ранее полученных утверждений) новых верных утверждений.
    \item [Виртуализация] --- получение знаний о целом или его частях с помощью искусственной среды, ситуации.
\end{description}

\subsubsection{Эмпирико-теоретические методы}

Эмпирико-теоретические методы (смешанные) или методы построения теорий на основе полученных эмпирических данных об объекте, процессе, явлении.

\begin{itemize}
    \item \textbf{Абстрагирование} --- выделение наиболее важных для исследования свойств, сторон исследуемого объекта, процесса, явления и игнорирование несущественных и второстепенных.
    \item \textbf{Анализ} --- разъединение целого на части с целью выявления их связей.
    \item \textbf{Декомпозиция} --- разъединение целого на части с сохранением их связей с окружением.
    \item \textbf{Синтез} --- соединение частей в целое с целью выявления их взаимосвязей.
    \item \textbf{Композиция} --- соединение частей целого с сохранением их взаимосвязей с окружением.
    \item \textbf{Индукция} --- получение знания о целом по знаниям о частях.
    \item \textbf{Дедукция} --- получение знания о частях по знаниям о целом.
    \item \textbf{Эвристики, использование эвристических процедур} --- получение знания о целом по знаниям о частях и по наблюдениям, опыту, интуиции, предвидению.
    \item \textbf{Моделирование (простое моделирование)}, использование приборов -- получение знания о целом или о его частях с помощью модели или приборов.
    \item \textbf{Исторический метод} --- поиск знаний с использованием предыстории, реально существовавшей или же мыслимой.
    \item \textbf{Логический метод} --- поиск знаний путем воспроизведения частей, связей или элементов в мышлении.
    \item \textbf{Макетирование} --- получение информации по макету, представлению частей в упрощенном, но целостном виде.
    \item \textbf{Актуализация} --- получение информации с помощью перевода целого или его частей (а следовательно, и целого) из статического состояния в динамическое состояние.
    \item \textbf{Визуализация} --- получение информации с помощью наглядного или визуального представления состояний объекта, процесса, явления.
\end{itemize}

\noindent
Кроме указанных классических форм реализации теоретико-эмпирических методов часто используются и мониторинг (система наблюдений и анализа состояний), деловые игры и ситуации, экспертные оценки (экспертное оценивание), имитация (подражание) и другие формы.

\example{1}

\noindent
Для построения модели планирования и управления производством в рамках страны, региона или крупной отрасли нужно решить следующие проблемы:
\begin{enumerate}
    \item Определить структурные связи, уровни управления и принятия решений, ресурсы; при этом чаще используются методы наблюдения, сравнения, измерения, эксперимента, анализа и синтеза, дедукции и индукции, эвристический, исторический и логический методы, макетирование и др.;
    \item Определить гипотезы, цели, возможные проблемы планирования; наиболее используемые методы --- наблюдение, сравнение, эксперимент, абстрагирование, анализ, синтез, дедукция, индукция, эвристический, исторический, логический и др.;
    \item Конструирование эмпирических моделей; наиболее используемые методы --- абстрагирование, анализ, синтез, индукция, дедукция, формализация, идеализация и др.;
    \item Поиск решения проблемы планирования и просчет различных вариантов, директив планирования, поиск оптимального решения; используемые чаще методы – измерение, сравнение, эксперимент, анализ, синтез, индукция, дедукция, актуализация, макетирование, визуализация, виртуализация и др.
\end{enumerate}
